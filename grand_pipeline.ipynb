{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk",
    "tags": []
   },
   "source": [
    "## First Time Installation\n",
    "\n",
    "Install the latest version of Vertex SDK for Python. First time in instance only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install_aip:mbsdk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Google Cloud Notebook\n",
    "# if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "#     USER_FLAG = \"--user\"\n",
    "# else:\n",
    "#     USER_FLAG = \"\"\n",
    "\n",
    "# ! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG\n",
    "# ! pip3 install -U google-cloud-storage $USER_FLAG\n",
    "# ! pip3 install $USER kfp google-cloud-pipeline-components --upgrade\n",
    "# ! pip3 install $USER icecream==2.1.1 --upgrade\n",
    "# ! pip3 install $USER pandas-gbq==0.15.0 --upgrade\n",
    "# ! pip3 install $USER google-cloud-secret-manager --upgrade\n",
    "\n",
    "# import os\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     # Automatically restart kernel after installs\n",
    "#     import IPython\n",
    "\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)\n",
    "\n",
    "# # Check versions\n",
    "# ! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "# ! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load your pipeline components and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| SERVICE_ACCOUNT: '751015570376-compute@developer.gserviceaccount.com'\n",
      "ic| PROJECT_ID: 'airasia-gaexport'\n",
      "ic| REGION: 'us-central1'\n",
      "ic| PIPELINE_NAME: 'rowcount-forecast-vaip-prod'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_NAME exists : gs://rowcount-forecast-vaip-prod\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Related Python packages\n",
    "from icecream import ic\n",
    "from kfp.v2 import dsl\n",
    "# from kfp import dsl as dsl\n",
    "\n",
    "# Components\n",
    "from ml_components.dataimport import (\n",
    "    get_rundates,\n",
    "    get_period,\n",
    "    get_import_query,\n",
    "    bq_query_no_return,\n",
    "    bq_query_to_dataframe,\n",
    "    get_previous_dataset_from_model_league,\n",
    "    \n",
    ")\n",
    "from ml_components.datacheck import (\n",
    "#     printing,\n",
    "    grand_drift_check,\n",
    ")\n",
    "from ml_components.datapreproc import (\n",
    "    data_preprocess,\n",
    "    data_interpolate,\n",
    ")\n",
    "from ml_components.modelling import (\n",
    "    modelling,\n",
    ")\n",
    "\n",
    "from ml_components.prediction import (\n",
    "    prediction,\n",
    ")\n",
    "\n",
    "from ml_components.outbound import (\n",
    "    data_collection,\n",
    "    html_collection,\n",
    "    generate_bq_table_from_gsc,\n",
    "    update_model_league,\n",
    ")\n",
    "from ml_components.alert import (\n",
    "    push_slack_notification,\n",
    ")\n",
    "from ml_components.pipelinehelper import (\n",
    "    func_op,\n",
    "    save_pipeline,\n",
    "    run_pipeline,\n",
    ")\n",
    "\n",
    "# Configs\n",
    "from config import (\n",
    "    SERVICE_ACCOUNT,\n",
    "    PROJECT_ID,\n",
    "    REGION,\n",
    "    RUNNER,\n",
    "    PIPELINE_NAME,\n",
    "    BUCKET_NAME,\n",
    "    E2E_PIPELINE_ROOT as PIPELINE_ROOT,\n",
    "    PARAMETER_VALUES,\n",
    "    parameter_checks\n",
    ")\n",
    "parameter_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your pipeline\n",
    "```\n",
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def your_pipeline(\n",
    "    parameter_value: data_type,\n",
    "):\n",
    "    ops_1 = func_op(\n",
    "        func=function,\n",
    "        _component_human_name='your ops 1 label',\n",
    "        base_image='python:3.7', #optional\n",
    "        function_arg=parameter_value,\n",
    "    )\n",
    "    \n",
    "    ops_2 = func_op(\n",
    "        func=function,\n",
    "        _component_human_name='your ops 2 label',\n",
    "        packages_to_install=[\n",
    "            'pandas==1.3.3'\n",
    "        ],\n",
    "        function_arg=ops_1.outputs['output'],\n",
    "    )\n",
    "```\n",
    "\n",
    "#### Choose your desired GCP pre-built image for your node(s)\n",
    "- https://cloud.google.com/deep-learning-containers/docs/choosing-container\n",
    "- `! gcloud container images list --repository=\"gcr.io/deeplearning-platform-release\"`\n",
    "- You can also just choose 'python:3.x' if want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def grand_pipeline(\n",
    "    bucket_name: str,\n",
    "    project_id: str,\n",
    "    project_name: str,\n",
    "    region: str,\n",
    "    job_id: str,\n",
    "    run_date: str,\n",
    "    train_size: float,\n",
    "    training_months: int,\n",
    "    sku_column: str,\n",
    "    ts_column: str,\n",
    "    val_column: str,\n",
    "    metrics_name: str,\n",
    "    freq: str,\n",
    "    allowed_models: str,\n",
    "    feature_importance_dict_str: str,\n",
    "    numerical_drift_partition_threshold: float,\n",
    "    numerical_importance_partition_threshold: float,\n",
    "    categorical_drift_partition_threshold: float,\n",
    "    categorical_importance_partition_threshold: float,\n",
    "    category_threshold: int,\n",
    "    delta: int,\n",
    "    forecasting_horizon_days: int,\n",
    "    model_params: str,\n",
    "    runner: str,\n",
    "    tracking_cutoff: int,\n",
    "):\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "    ################################################### Ops Declaration ####################################################\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "\n",
    "    \n",
    "    ###########################################################\n",
    "    ######################### Rundate #########################\n",
    "    ###########################################################\n",
    "\n",
    "    rundates_op = func_op(\n",
    "        func=get_rundates,\n",
    "        _component_human_name='get_rundates',\n",
    "        base_image='python:3.7',\n",
    "        packages_to_install=['pandas==1.3.3'],\n",
    "        run_date=run_date,\n",
    "    )\n",
    "\n",
    "    usage_run_date = rundates_op.outputs['usage_run_date']\n",
    "    src_run_dt = rundates_op.outputs['src_run_dt']\n",
    "\n",
    "    ###########################################################\n",
    "    ####################### Import Data #######################\n",
    "    ###########################################################\n",
    "\n",
    "    period_op = func_op(\n",
    "        func=get_period,\n",
    "        _component_human_name='get_period',\n",
    "        base_image='python:3.7',\n",
    "        packages_to_install=['pandas==1.3.3'],\n",
    "        run_date=usage_run_date,\n",
    "        training_months=training_months,\n",
    "    )\n",
    "\n",
    "    # Get queries\n",
    "    import_train_data_query_op = func_op(\n",
    "        func=get_import_query,\n",
    "        _component_human_name='get_train_import_query',\n",
    "        base_image='python:3.7',\n",
    "        start_date=period_op.outputs['train_start_date'],\n",
    "        end_date=period_op.outputs['train_end_date'],\n",
    "    )\n",
    "\n",
    "    # Get datasets using queries\n",
    "    train_dataset_op = func_op(\n",
    "        func=bq_query_to_dataframe,\n",
    "        _component_human_name='get_train_dataset',\n",
    "        base_image='python:3.7',\n",
    "        project_id=project_id,\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        query=import_train_data_query_op.outputs['query'],\n",
    "    )\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    #################### Data Preprocessing ###################\n",
    "    ###########################################################\n",
    "\n",
    "    train_data_preprocess_op = func_op(\n",
    "        func=data_preprocess,\n",
    "        _component_human_name='train_data_preprocess',\n",
    "        base_image='python:3.7',\n",
    "        packages_to_install=['pandas==1.3.3'],\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        sku_column=sku_column,\n",
    "        ts_column=ts_column,\n",
    "        val_column=val_column,\n",
    "        dataset=train_dataset_op.outputs['dataset'],\n",
    "    )\n",
    "\n",
    "\n",
    "    parallel_op = dsl.ParallelFor(\n",
    "        train_data_preprocess_op.outputs['train_dataset_path'], \n",
    "        parallelism=1000000,\n",
    "    )\n",
    "    with parallel_op as table:\n",
    "        # print_op = func_op(\n",
    "        #     func=printing,\n",
    "        #     _component_human_name='print',\n",
    "        #     base_image='python:3.7',\n",
    "        #     msg=table.sku_name,\n",
    "        # )\n",
    "\n",
    "        ###########################################################\n",
    "        #################### Data Interpolation ###################\n",
    "        ###########################################################\n",
    "\n",
    "        interpolate_op = func_op(\n",
    "            func=data_interpolate,\n",
    "            _component_human_name=f'interpolate',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            packages_to_install=[\n",
    "                'darts==0.19.0',\n",
    "            ],\n",
    "            cpu_limit='2',\n",
    "            memory_limit='8G',\n",
    "            sku_name=table.sku_name,\n",
    "            train_start_date=period_op.outputs['train_start_date'],\n",
    "            train_end_date=period_op.outputs['train_end_date'],\n",
    "            freq=freq,\n",
    "            sku_column=sku_column,\n",
    "            ts_column=ts_column,\n",
    "            val_column=val_column,\n",
    "            processed_dataset=train_data_preprocess_op.outputs['processed_dataset'],\n",
    "        )\n",
    "\n",
    "        ###########################################################\n",
    "        #################### Data Drift Check #####################\n",
    "        ###########################################################\n",
    "\n",
    "        # Get previous datasets using model league\n",
    "        prev_train_dataset_op = func_op(\n",
    "            func=get_previous_dataset_from_model_league,\n",
    "            _component_human_name='prev_train_dataset',\n",
    "            base_image='python:3.7',\n",
    "            packages_to_install=['pandas-gbq==0.15.0'],\n",
    "            cpu_limit='2',\n",
    "            memory_limit='4G',\n",
    "            retry=3,\n",
    "            project_id=project_id,\n",
    "            project_name=project_name,\n",
    "            runner=runner,\n",
    "            sku_column=sku_column,\n",
    "            ts_column=ts_column,\n",
    "            val_column=val_column,\n",
    "            sku_name=table.sku_name,\n",
    "            location=region,\n",
    "        )\n",
    "\n",
    "        drift_check_op = func_op(\n",
    "            func=grand_drift_check,\n",
    "            _component_human_name='drift_check',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            cpu_limit='2',\n",
    "            memory_limit='4G',\n",
    "            bucket_name=bucket_name,\n",
    "            src_run_dt=src_run_dt,\n",
    "            sku_column=sku_column,\n",
    "            sku_name=table.sku_name,\n",
    "            dataset_p=interpolate_op.outputs['interpolated_dataset'],\n",
    "            dataset_q=prev_train_dataset_op.outputs['previous_dataset'],\n",
    "            feature_importance_dict_str=feature_importance_dict_str,\n",
    "            numerical_drift_partition_threshold=numerical_drift_partition_threshold,\n",
    "            numerical_importance_partition_threshold=numerical_importance_partition_threshold,\n",
    "            categorical_drift_partition_threshold=categorical_drift_partition_threshold,\n",
    "            categorical_importance_partition_threshold=categorical_importance_partition_threshold,\n",
    "            category_threshold=category_threshold,\n",
    "            delta=delta,\n",
    "        )\n",
    "\n",
    "        ###########################################################\n",
    "        ######################## Modelling ########################\n",
    "        ###########################################################\n",
    "\n",
    "        modelling_op = func_op(\n",
    "            func=modelling,\n",
    "            _component_human_name='modelling',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            packages_to_install=[\n",
    "                'darts==0.19.0',\n",
    "                'pandas-gbq==0.15.0',\n",
    "            ],\n",
    "            cpu_limit='4',\n",
    "            memory_limit='4G',\n",
    "            drift_status=drift_check_op.outputs['drift_status'],\n",
    "            project_id=project_id,\n",
    "            project_name=project_name,\n",
    "            runner=runner,\n",
    "            bucket_name=bucket_name,\n",
    "            src_run_dt=src_run_dt,\n",
    "            sku_name=table.sku_name,\n",
    "            train_size=train_size,\n",
    "            sku_column=sku_column,\n",
    "            ts_column=ts_column,\n",
    "            val_column=val_column,\n",
    "            allowed_models=allowed_models,\n",
    "            model_params=model_params,\n",
    "            previous_train_dataset=prev_train_dataset_op.outputs['previous_train_dataset'],\n",
    "            previous_val_dataset=prev_train_dataset_op.outputs['previous_val_dataset'],\n",
    "            interpolated_dataset=interpolate_op.outputs['interpolated_dataset'],\n",
    "        )\n",
    "\n",
    "        ###########################################################\n",
    "        ######################## Prediction #######################\n",
    "        ###########################################################\n",
    "\n",
    "        prediction_op = func_op(\n",
    "            func=prediction,\n",
    "            _component_human_name='prediction',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            packages_to_install=[\n",
    "                'darts==0.19.0',\n",
    "            ],\n",
    "            cpu_limit='4',\n",
    "            memory_limit='4G',\n",
    "            bucket_name=bucket_name,\n",
    "            src_run_dt=src_run_dt,\n",
    "            sku_name=table.sku_name,\n",
    "            freq=freq,\n",
    "            sku_column=sku_column,\n",
    "            ts_column=ts_column,\n",
    "            val_column=val_column,\n",
    "            metrics_name=metrics_name,\n",
    "            model_params=model_params,\n",
    "            forecasting_horizon_days=forecasting_horizon_days,\n",
    "            interpolated_dataset=interpolate_op.outputs['interpolated_dataset'],\n",
    "            accuracy_board_dataset=modelling_op.outputs['accuracy_board_dataset'],\n",
    "        )\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    ######################### Outbound ########################\n",
    "    ###########################################################\n",
    "\n",
    "    ################################\n",
    "    ########## Collection ##########\n",
    "    ################################\n",
    "\n",
    "    html_collection_op = func_op(\n",
    "        func=html_collection,\n",
    "        _component_human_name=f'html_collection',\n",
    "        base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        bucket_name=bucket_name,\n",
    "        src_run_dt=src_run_dt,\n",
    "    )\n",
    "    html_collection_op.after(parallel_op)\n",
    "\n",
    "    accuracy_board_collection_op = func_op(\n",
    "        func=data_collection,\n",
    "        _component_human_name=f'accuracy_board_collection',\n",
    "        base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        bucket_name=bucket_name,\n",
    "        src_run_dt=src_run_dt,\n",
    "        mode='board',\n",
    "    )\n",
    "    accuracy_board_collection_op.after(parallel_op)\n",
    "\n",
    "    train_collection_op = func_op(\n",
    "        func=data_collection,\n",
    "        _component_human_name=f'train_collection',\n",
    "        base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        bucket_name=bucket_name,\n",
    "        src_run_dt=src_run_dt,\n",
    "        mode='train',\n",
    "    )\n",
    "    train_collection_op.after(parallel_op)\n",
    "\n",
    "    val_collection_op = func_op(\n",
    "        func=data_collection,\n",
    "        _component_human_name=f'val_collection',\n",
    "        base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        bucket_name=bucket_name,\n",
    "        src_run_dt=src_run_dt,\n",
    "        mode='val',\n",
    "    )\n",
    "    val_collection_op.after(parallel_op)\n",
    "\n",
    "    prediction_collection_op = func_op(\n",
    "        func=data_collection,\n",
    "        _component_human_name=f'prediction_collection',\n",
    "        base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        bucket_name=bucket_name,\n",
    "        src_run_dt=src_run_dt,\n",
    "        mode='prediction',\n",
    "    )\n",
    "    prediction_collection_op.after(parallel_op)\n",
    "\n",
    "\n",
    "    ################################\n",
    "    ########## BQ Tracking #########\n",
    "    ################################\n",
    "\n",
    "    board_persist_to_bq_op = func_op(\n",
    "        func=generate_bq_table_from_gsc,\n",
    "        _component_human_name='board_persist_to_bq',\n",
    "        base_image='python:3.7',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        retry=3,\n",
    "        project_id=project_id,\n",
    "        project_name=project_name,\n",
    "        dataset_id='MLOPS_TRACKING',\n",
    "        runner=runner,\n",
    "        table_name='accuracy_board',\n",
    "        src_run_dt='',\n",
    "        dataset_format='PARQUET',\n",
    "        location=region,\n",
    "        dataset_to_save=accuracy_board_collection_op.outputs['grand_dataset'],\n",
    "    )\n",
    "\n",
    "    train_dataset_to_bq_op = func_op(\n",
    "        func=generate_bq_table_from_gsc,\n",
    "        _component_human_name='train_dataset_to_bq',\n",
    "        base_image='python:3.7',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        retry=3,\n",
    "        project_id=project_id,\n",
    "        project_name=project_name,\n",
    "        dataset_id='MLOPS_TRAIN_DATASET',\n",
    "        runner=runner,\n",
    "        table_name='train',\n",
    "        src_run_dt=src_run_dt,\n",
    "        dataset_format='PARQUET',\n",
    "        location=region,\n",
    "        dataset_to_save=train_collection_op.outputs['grand_dataset'],\n",
    "    )\n",
    "\n",
    "    train_persist_to_bq_op = func_op(\n",
    "        func=generate_bq_table_from_gsc,\n",
    "        _component_human_name='train_persist_to_bq',\n",
    "        base_image='python:3.7',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        retry=3,\n",
    "        project_id=project_id,\n",
    "        project_name=project_name,\n",
    "        dataset_id='MLOPS_TRAIN_DATASET',\n",
    "        runner=runner,\n",
    "        table_name='train',\n",
    "        src_run_dt='',\n",
    "        dataset_format='PARQUET',\n",
    "        location=region,\n",
    "        dataset_to_save=train_collection_op.outputs['grand_dataset'],\n",
    "    )\n",
    "\n",
    "    val_dataset_to_bq_op = func_op(\n",
    "        func=generate_bq_table_from_gsc,\n",
    "        _component_human_name='val_dataset_to_bq',\n",
    "        base_image='python:3.7',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        retry=3,\n",
    "        project_id=project_id,\n",
    "        project_name=project_name,\n",
    "        dataset_id='MLOPS_VAL_DATASET',\n",
    "        runner=runner,\n",
    "        table_name='val',\n",
    "        src_run_dt=src_run_dt,\n",
    "        dataset_format='PARQUET',\n",
    "        location=region,\n",
    "        dataset_to_save=val_collection_op.outputs['grand_dataset'],\n",
    "    )\n",
    "\n",
    "    val_persist_to_bq_op = func_op(\n",
    "        func=generate_bq_table_from_gsc,\n",
    "        _component_human_name='val_persist_to_bq',\n",
    "        base_image='python:3.7',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        retry=3,\n",
    "        project_id=project_id,\n",
    "        project_name=project_name,\n",
    "        dataset_id='MLOPS_VAL_DATASET',\n",
    "        runner=runner,\n",
    "        table_name='val',\n",
    "        src_run_dt='',\n",
    "        dataset_format='PARQUET',\n",
    "        location=region,\n",
    "        dataset_to_save=val_collection_op.outputs['grand_dataset'],\n",
    "    )\n",
    "\n",
    "    prediction_dataset_to_bq_op = func_op(\n",
    "        func=generate_bq_table_from_gsc,\n",
    "        _component_human_name='prediction_dataset_to_bq',\n",
    "        base_image='python:3.7',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        retry=3,\n",
    "        project_id=project_id,\n",
    "        project_name=project_name,\n",
    "        dataset_id='MLOPS_PREDICTION_DATASET',\n",
    "        runner=runner,\n",
    "        table_name='prediction',\n",
    "        src_run_dt=src_run_dt,\n",
    "        dataset_format='PARQUET',\n",
    "        location=region,\n",
    "        dataset_to_save=prediction_collection_op.outputs['grand_dataset'],\n",
    "    )\n",
    "\n",
    "    prediction_persist_to_bq_op = func_op(\n",
    "        func=generate_bq_table_from_gsc,\n",
    "        _component_human_name='prediction_persist_to_bq',\n",
    "        base_image='python:3.7',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        retry=3,\n",
    "        project_id=project_id,\n",
    "        project_name=project_name,\n",
    "        dataset_id='MLOPS_PREDICTION_DATASET',\n",
    "        runner=runner,\n",
    "        table_name='prediction',\n",
    "        src_run_dt='',\n",
    "        dataset_format='PARQUET',\n",
    "        location=region,\n",
    "        dataset_to_save=prediction_collection_op.outputs['grand_dataset'],\n",
    "    )\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    ########## Update Model League ##########\n",
    "    #########################################\n",
    "\n",
    "    update_model_league_op = func_op(\n",
    "        func=update_model_league,\n",
    "        _component_human_name='update_model_league',\n",
    "        base_image='python:3.7',\n",
    "        packages_to_install=['pandas-gbq==0.15.0'],\n",
    "        cpu_limit='2',\n",
    "        memory_limit='8G',\n",
    "        project_id=project_id,\n",
    "        project_name=project_name,\n",
    "        location=region,\n",
    "        job_id=job_id,\n",
    "        sku_column=sku_column,\n",
    "        metrics_name=metrics_name,\n",
    "        src_run_dt=src_run_dt,\n",
    "        train_bq_path=train_dataset_to_bq_op.outputs['bq_path'],\n",
    "        val_bq_path=val_dataset_to_bq_op.outputs['bq_path'],\n",
    "        prediction_bq_path=prediction_dataset_to_bq_op.outputs['bq_path'],\n",
    "        runner=runner,\n",
    "        tracking_cutoff=tracking_cutoff,\n",
    "        accuracy_board_dataset=accuracy_board_collection_op.outputs['grand_dataset'],\n",
    "    )\n",
    "\n",
    "\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "    ##################################################### Ops Caching ######################################################\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "\n",
    "    # pr = persist_run\n",
    "    nodes_no_cache = [\n",
    "        rundates_op, #pr\n",
    "\n",
    "#         period_op,\n",
    "#         import_train_data_query_op,\n",
    "        # train_dataset_op,\n",
    "        # train_data_preprocess_op,\n",
    "\n",
    "        prev_train_dataset_op, #pr\n",
    "#         drift_check_op,\n",
    "\n",
    "        html_collection_op,\n",
    "        accuracy_board_collection_op,\n",
    "        train_collection_op,\n",
    "        val_collection_op,\n",
    "        prediction_collection_op,\n",
    "\n",
    "        # train_dataset_to_bq_op,\n",
    "        # val_dataset_to_bq_op,\n",
    "        # prediction_dataset_to_bq_op,\n",
    "\n",
    "        board_persist_to_bq_op, #pr\n",
    "        train_persist_to_bq_op, #pr\n",
    "        val_persist_to_bq_op, #pr\n",
    "        prediction_persist_to_bq_op, #pr\n",
    "        \n",
    "        update_model_league_op, #pr\n",
    "    ]\n",
    "    \n",
    "    if nodes_no_cache:\n",
    "        for node in nodes_no_cache:\n",
    "            node.set_caching_options(enable_caching=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:775: UserWarning: Setting parallelism in ParallelFor is not supported yet.The setting is ignored.\n",
      "  'Setting parallelism in ParallelFor is not supported yet.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_spec/rowcount-forecast-vaip-prod-2022-05-26-t-13-17-44.json uploaded to GCS.\n",
      "/tmp/rowcount-forecast-vaip-prod-2022-05-26-t-13-17-44.json removed in local.\n"
     ]
    }
   ],
   "source": [
    "TEMPLATE_PATH, JOB_ID, DISPLAY_NAME = save_pipeline(\n",
    "    pipeline=grand_pipeline,\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    mode=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your pipeline locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/751015570376/locations/us-central1/pipelineJobs/rowcount-forecast-vaip-prod-2022-05-26-t-13-17-44\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/751015570376/locations/us-central1/pipelineJobs/rowcount-forecast-vaip-prod-2022-05-26-t-13-17-44')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/rowcount-forecast-vaip-prod-2022-05-26-t-13-17-44?project=751015570376\n",
      "PipelineJob projects/751015570376/locations/us-central1/pipelineJobs/rowcount-forecast-vaip-prod-2022-05-26-t-13-17-44 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/751015570376/locations/us-central1/pipelineJobs/rowcount-forecast-vaip-prod-2022-05-26-t-13-17-44 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/751015570376/locations/us-central1/pipelineJobs/rowcount-forecast-vaip-prod-2022-05-26-t-13-17-44 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(\n",
    "    project_id=PROJECT_ID,\n",
    "    staging_bucket=BUCKET_NAME,\n",
    "    location=REGION,\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=TEMPLATE_PATH,\n",
    "    job_id=JOB_ID,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    parameter_values=PARAMETER_VALUES,\n",
    "    # disable_caching_all=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lightweight_functions_component_io_kfp.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
