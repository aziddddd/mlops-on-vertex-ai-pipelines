{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## First Time Installation\n",
    "\n",
    "Install the latest version of Vertex SDK for Python. First time in instance only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install_aip:mbsdk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Google Cloud Notebook\n",
    "# if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "#     USER_FLAG = \"--user\"\n",
    "# else:\n",
    "#     USER_FLAG = \"\"\n",
    "\n",
    "# ! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG\n",
    "# ! pip3 install -U google-cloud-storage $USER_FLAG\n",
    "# ! pip3 install $USER kfp google-cloud-pipeline-components --upgrade\n",
    "# ! pip3 install $USER icecream==2.1.1 --upgrade\n",
    "# ! pip3 install $USER pandas-gbq==0.15.0 --upgrade\n",
    "# ! pip3 install $USER google-cloud-secret-manager --upgrade\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     # Automatically restart kernel after installs\n",
    "#     import IPython\n",
    "\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)\n",
    "\n",
    "# # Check versions\n",
    "# ! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "# ! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load your pipeline components and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| SERVICE_ACCOUNT: '751015570376-compute@developer.gserviceaccount.com'\n",
      "ic| PROJECT_ID: 'airasia-gaexport'\n",
      "ic| REGION: 'us-central1'\n",
      "ic| PIPELINE_NAME: 'fraudy-classify-vaip-dev'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_NAME exists : gs://fraudy-classify-vaip-dev\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Related Python packages\n",
    "from icecream import ic\n",
    "from kfp.v2 import dsl\n",
    "\n",
    "# Components\n",
    "from ml_components.dataimport import (\n",
    "    get_rundates,\n",
    "    get_period,\n",
    "    get_import_query,\n",
    "    bq_query_no_return,\n",
    "    bq_query_to_dataframe,\n",
    "    get_previous_dataset_from_model_league,\n",
    "    \n",
    ")\n",
    "from ml_components.datacheck import (\n",
    "    printing,\n",
    "    grand_drift_check,\n",
    ")\n",
    "from ml_components.datapreproc import (\n",
    "    data_preprocess,\n",
    "    data_split,\n",
    ")\n",
    "from ml_components.modelling import (\n",
    "    model_trainer,\n",
    "    get_champion_model,\n",
    "    model_evaluator,\n",
    ")\n",
    "from ml_components.outbound import (\n",
    "    generate_bq_table_from_gsc,\n",
    "    update_model_league,\n",
    ")\n",
    "from ml_components.alert import (\n",
    "    push_slack_notification,\n",
    ")\n",
    "from ml_components.pipelinehelper import (\n",
    "    func_op,\n",
    "    save_pipeline,\n",
    "    run_pipeline,\n",
    ")\n",
    "\n",
    "# Configs\n",
    "from config import (\n",
    "    SERVICE_ACCOUNT,\n",
    "    PROJECT_ID,\n",
    "    REGION,\n",
    "    RUNNER,\n",
    "    PIPELINE_NAME,\n",
    "    BUCKET_NAME,\n",
    "    TRAIN_PIPELINE_ROOT as PIPELINE_ROOT,\n",
    "    PARAMETER_VALUES,\n",
    "    parameter_checks\n",
    ")\n",
    "parameter_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your pipeline\n",
    "```\n",
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def your_pipeline(\n",
    "    parameter_value: data_type,\n",
    "):\n",
    "    ops_1 = func_op(\n",
    "        func=function,\n",
    "        _component_human_name='your ops 1 label',\n",
    "        base_image='python:3.7', #optional\n",
    "        function_arg=parameter_value,\n",
    "    )\n",
    "    \n",
    "    ops_2 = func_op(\n",
    "        func=function,\n",
    "        _component_human_name='your ops 2 label',\n",
    "        packages_to_install=[\n",
    "            'pandas==1.3.3'\n",
    "        ],\n",
    "        function_arg=ops_1.outputs['output'],\n",
    "    )\n",
    "```\n",
    "\n",
    "#### Choose your desired GCP pre-built image for your node(s)\n",
    "- https://cloud.google.com/deep-learning-containers/docs/choosing-container\n",
    "- `! gcloud container images list --repository=\"gcr.io/deeplearning-platform-release\"`\n",
    "- You can also just choose 'python:3.x' if want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def grand_pipeline(\n",
    "    bucket_name: str,\n",
    "    project_id: str,\n",
    "    project_name: str,\n",
    "    region: str,\n",
    "    job_id: str,\n",
    "    run_date: str,\n",
    "    seed: int,\n",
    "    train_size: float,\n",
    "    target_column: str,\n",
    "    drop_columns: str,\n",
    "    auto_balance: str,\n",
    "    feature_importance_dict_str: str,\n",
    "    numerical_drift_partition_threshold: float,\n",
    "    numerical_importance_partition_threshold: float,\n",
    "    categorical_drift_partition_threshold: float,\n",
    "    categorical_importance_partition_threshold: float,\n",
    "    category_threshold: int,\n",
    "    delta: int,\n",
    "    model_params: str,\n",
    "    is_endpoint: bool,\n",
    "    mlops_topic: str,\n",
    "    runner: str,\n",
    "):\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "    ################################################### Ops Declaration ####################################################\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "\n",
    "    \n",
    "    ###########################################################\n",
    "    ######################### Rundate #########################\n",
    "    ###########################################################\n",
    "\n",
    "    rundates_op = func_op(\n",
    "        func=get_rundates,\n",
    "        _component_human_name='get_rundates',\n",
    "        base_image='python:3.7',\n",
    "        packages_to_install=['pandas==1.3.3'],\n",
    "        run_date=run_date,\n",
    "    )\n",
    "\n",
    "    usage_run_date = rundates_op.outputs['usage_run_date']\n",
    "    src_run_dt = rundates_op.outputs['src_run_dt']\n",
    "\n",
    "    ###########################################################\n",
    "    ####################### Import Data #######################\n",
    "    ###########################################################\n",
    "\n",
    "    period_op = func_op(\n",
    "        func=get_period,\n",
    "        _component_human_name='get_period',\n",
    "        base_image='python:3.7',\n",
    "        packages_to_install=['pandas==1.3.3'],\n",
    "        run_date=usage_run_date,\n",
    "    )\n",
    "\n",
    "    # Get queries\n",
    "    import_train_data_query_op = func_op(\n",
    "        func=get_import_query,\n",
    "        _component_human_name='get_train_import_query',\n",
    "        base_image='python:3.7',\n",
    "        datestr=period_op.outputs['train_dt'],\n",
    "    )\n",
    "\n",
    "    # Get previous datasets using model league\n",
    "    prev_train_dataset_op = func_op(\n",
    "        func=get_previous_dataset_from_model_league,\n",
    "        _component_human_name='get_prev_train_dataset',\n",
    "        base_image='python:3.7',\n",
    "        packages_to_install=['pandas-gbq==0.15.0'],\n",
    "        cpu_limit='2',\n",
    "        memory_limit='32G',\n",
    "        project_id=project_id,\n",
    "        project_name=project_name,\n",
    "        dataset_type='train',\n",
    "        runner=runner,\n",
    "        location=region,\n",
    "    )\n",
    "\n",
    "    # Get datasets using queries\n",
    "    train_dataset_op = func_op(\n",
    "        func=bq_query_to_dataframe,\n",
    "        _component_human_name='get_train_dataset',\n",
    "        base_image='python:3.7',\n",
    "        project_id=project_id,\n",
    "        cpu_limit='2',\n",
    "        memory_limit='32G',\n",
    "        query=import_train_data_query_op.outputs['query'],\n",
    "    )\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    #################### Data Preprocessing ###################\n",
    "    ###########################################################\n",
    "\n",
    "    train_data_preprocess_op = func_op(\n",
    "        func=data_preprocess,\n",
    "        _component_human_name='train_data_preprocess',\n",
    "        base_image='python:3.7',\n",
    "        packages_to_install=['pandas==1.3.3'],\n",
    "        cpu_limit='4',\n",
    "        memory_limit='32G',\n",
    "        drop_columns=drop_columns,\n",
    "        dataset=train_dataset_op.outputs['dataset'],\n",
    "    )\n",
    "\n",
    "    data_split_op = func_op(\n",
    "        func=data_split,\n",
    "        _component_human_name='data_split',\n",
    "        base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "        packages_to_install=[\n",
    "            'imbalanced-learn==0.9.0',\n",
    "        ],\n",
    "        cpu_limit='4',\n",
    "        memory_limit='32G',\n",
    "        auto_balance=auto_balance,\n",
    "        category_threshold=category_threshold,\n",
    "        seed=seed,\n",
    "        train_size=train_size,\n",
    "        target_column=target_column,\n",
    "        processed_dataset=train_data_preprocess_op.outputs['processed_dataset'],\n",
    "    )\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    #################### Data Drift Check #####################\n",
    "    ###########################################################\n",
    "\n",
    "    train_drift_check_op = func_op(\n",
    "        func=grand_drift_check,\n",
    "        _component_human_name='train_drift_check',\n",
    "        base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "        cpu_limit='4',\n",
    "        memory_limit='32G',\n",
    "        bucket_name=bucket_name,\n",
    "        src_run_dt=src_run_dt,\n",
    "        project_name=project_name,\n",
    "        dataset_p=data_split_op.outputs['train_dataset'],\n",
    "        dataset_q=prev_train_dataset_op.outputs['previous_dataset'],\n",
    "        feature_importance_dict_str=feature_importance_dict_str,\n",
    "        numerical_drift_partition_threshold=numerical_drift_partition_threshold,\n",
    "        numerical_importance_partition_threshold=numerical_importance_partition_threshold,\n",
    "        categorical_drift_partition_threshold=categorical_drift_partition_threshold,\n",
    "        categorical_importance_partition_threshold=categorical_importance_partition_threshold,\n",
    "        category_threshold=category_threshold,\n",
    "        delta=delta,\n",
    "        mode='train',\n",
    "    )\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    ######################## Modelling ########################\n",
    "    ###########################################################\n",
    "\n",
    "    with dsl.Condition(\n",
    "        train_drift_check_op.outputs['drift_status'] == 'false',  ## If got drift\n",
    "        name=\"modelling\"\n",
    "    ):\n",
    "    #     data_check_result_fail_op = func_op(\n",
    "    #         func=push_slack_notification,\n",
    "    #         _component_human_name='data_check_result_fail',\n",
    "    #         base_image='python:3.7',\n",
    "    #         packages_to_install=['google-cloud-secret-manager'],\n",
    "    #         job_id=job_id,\n",
    "    #         src_run_dt=src_run_dt,\n",
    "    #         text=data_check_result_op.outputs['alert_msg'],\n",
    "    #         channel='#your-project-name-internal',\n",
    "    #         webhook_config_str=failure_webhook_config,\n",
    "    #         runner=runner,\n",
    "    #     )\n",
    "\n",
    "        train_dataset_to_bq_op = func_op(\n",
    "            func=generate_bq_table_from_gsc,\n",
    "            _component_human_name='train_dataset_to_bq',\n",
    "            base_image='python:3.7',\n",
    "            cpu_limit='2',\n",
    "            memory_limit='8G',\n",
    "            retry=3,\n",
    "            project_id=project_id,\n",
    "            project_name=project_name,\n",
    "            dataset_id='MLOPS_TRAIN_DATASET',\n",
    "            runner=runner,\n",
    "            table_name='train',\n",
    "            src_run_dt=src_run_dt,\n",
    "            dataset_format='PARQUET',\n",
    "            location=region,\n",
    "            dataset_to_save=data_split_op.outputs['train_dataset'],\n",
    "        )\n",
    "\n",
    "        model_trainer_op = func_op(\n",
    "            func=model_trainer,\n",
    "            _component_human_name='model_trainer',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            packages_to_install=[\n",
    "                'xgboost',\n",
    "            ],\n",
    "            cpu_limit='4',\n",
    "            memory_limit='16G',\n",
    "            seed=seed,\n",
    "            model_params=model_params,\n",
    "            x_train_dataset=data_split_op.outputs['x_train_dataset'],\n",
    "            x_val_dataset=data_split_op.outputs['x_val_dataset'],\n",
    "            y_train_dataset=data_split_op.outputs['y_train_dataset'],\n",
    "            y_val_dataset=data_split_op.outputs['y_val_dataset'],\n",
    "        )\n",
    "\n",
    "        model_champion_op = func_op(\n",
    "            func=get_champion_model,\n",
    "            _component_human_name='get_champion_model',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            packages_to_install=[\n",
    "                'xgboost',\n",
    "                'pandas-gbq==0.15.0',\n",
    "            ],\n",
    "            project_id=project_id,\n",
    "            project_name=project_name,\n",
    "            runner=runner,\n",
    "            location=region,\n",
    "        )\n",
    "\n",
    "        model_evaluator_op = func_op(\n",
    "            func=model_evaluator,\n",
    "            _component_human_name='model_evaluator',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            packages_to_install=[\n",
    "                'xgboost',\n",
    "                'pandas-gbq==0.15.0',\n",
    "                'seaborn==0.11.2',\n",
    "                'shap==0.40.0',\n",
    "            ],\n",
    "            cpu_limit='4',\n",
    "            memory_limit='16G',\n",
    "            is_cold_start=model_champion_op.outputs['is_cold_start'],\n",
    "            model_object= model_trainer_op.outputs['model_object'],\n",
    "            model_champion_object=model_champion_op.outputs['model_champion_object'],\n",
    "            x_train_dataset=data_split_op.outputs['x_train_dataset'],\n",
    "            x_val_dataset=data_split_op.outputs['x_val_dataset'],\n",
    "            y_val_dataset=data_split_op.outputs['y_val_dataset'],\n",
    "        )\n",
    "\n",
    "\n",
    "        ###########################################################\n",
    "        ######################## Outbound #########################\n",
    "        ###########################################################\n",
    "\n",
    "        train_update_model_league_op = func_op(\n",
    "            func=update_model_league,\n",
    "            _component_human_name='train_update_model_league',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            packages_to_install=[\n",
    "                'pandas-gbq==0.15.0',\n",
    "                'google-cloud-pubsub==2.13.0',\n",
    "            ],\n",
    "            cpu_limit='4',\n",
    "            memory_limit='16G',\n",
    "            project_id=project_id,\n",
    "            project_name=project_name,\n",
    "            location=region,\n",
    "            job_id=job_id,\n",
    "            src_run_dt=src_run_dt,\n",
    "            bq_path=train_dataset_to_bq_op.outputs['bq_path'],\n",
    "            choose_model=model_evaluator_op.outputs['best_model'],\n",
    "            runner=runner,\n",
    "            is_cold_start=model_champion_op.outputs['is_cold_start'],\n",
    "            is_endpoint=is_endpoint,\n",
    "            mlops_topic=mlops_topic,\n",
    "            update_mode='train',\n",
    "            model_object=model_trainer_op.outputs['model_object'],\n",
    "        )\n",
    "\n",
    "\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "    ##################################################### Ops Caching ######################################################\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "\n",
    "    nodes_no_cache = [\n",
    "        rundates_op,\n",
    "\n",
    "#         period_op,\n",
    "#         import_train_data_query_op,\n",
    "\n",
    "#         train_dataset_op,\n",
    "#         train_data_preprocess_op,        \n",
    "#         data_split_op,\n",
    "\n",
    "        prev_train_dataset_op,\n",
    "\n",
    "#         train_drift_check_op,\n",
    "        \n",
    "#         data_check_result_op,\n",
    "\n",
    "        model_champion_op,\n",
    "\n",
    "#         model_trainer_op,\n",
    "#         model_evaluator_op,\n",
    "\n",
    "        train_update_model_league_op,\n",
    "    ]\n",
    "    \n",
    "    if nodes_no_cache:\n",
    "        for node in nodes_no_cache:\n",
    "            node.set_caching_options(enable_caching=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pipeline_spec/fraudy-classify-vaip-dev-train-2022-06-17-t-15-02-10.json uploaded to GCS.\n",
      "/tmp/fraudy-classify-vaip-dev-train-2022-06-17-t-15-02-10.json removed in local.\n"
     ]
    }
   ],
   "source": [
    "TEMPLATE_PATH, JOB_ID, DISPLAY_NAME = save_pipeline(\n",
    "    pipeline=grand_pipeline,\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    mode='train',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your pipeline locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/751015570376/locations/us-central1/pipelineJobs/fraudy-classify-vaip-dev-train-2022-06-17-t-15-02-10\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/751015570376/locations/us-central1/pipelineJobs/fraudy-classify-vaip-dev-train-2022-06-17-t-15-02-10')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/fraudy-classify-vaip-dev-train-2022-06-17-t-15-02-10?project=751015570376\n",
      "PipelineJob projects/751015570376/locations/us-central1/pipelineJobs/fraudy-classify-vaip-dev-train-2022-06-17-t-15-02-10 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/751015570376/locations/us-central1/pipelineJobs/fraudy-classify-vaip-dev-train-2022-06-17-t-15-02-10 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/751015570376/locations/us-central1/pipelineJobs/fraudy-classify-vaip-dev-train-2022-06-17-t-15-02-10 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/751015570376/locations/us-central1/pipelineJobs/fraudy-classify-vaip-dev-train-2022-06-17-t-15-02-10 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/751015570376/locations/us-central1/pipelineJobs/fraudy-classify-vaip-dev-train-2022-06-17-t-15-02-10 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(\n",
    "    project_id=PROJECT_ID,\n",
    "    staging_bucket=BUCKET_NAME,\n",
    "    location=REGION,\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=TEMPLATE_PATH,\n",
    "    job_id=JOB_ID,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    parameter_values=PARAMETER_VALUES,\n",
    "    # disable_caching_all=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lightweight_functions_component_io_kfp.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
