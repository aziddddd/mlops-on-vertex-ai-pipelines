{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk",
    "tags": []
   },
   "source": [
    "## First Time Installation\n",
    "\n",
    "Install the latest version of Vertex SDK for Python. First time in instance only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install_aip:mbsdk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip3 install $USER google-cloud-aiplatform==1.16.0 --upgrade\n",
    "# ! pip3 install $USER google-cloud-bigquery==2.34.4 --upgrade\n",
    "# ! pip3 install $USER google-cloud-bigquery-storage==2.13.2 --upgrade\n",
    "# ! pip3 install $USER google-cloud-storage==1.44.0 --upgrade\n",
    "# ! pip3 install $USER kfp==1.8.13 --upgrade\n",
    "# ! pip3 install $USER google-cloud-pipeline-components==1.0.14 --upgrade\n",
    "# ! pip3 install $USER icecream==2.1.1 --upgrade\n",
    "# ! pip3 install $USER pandas-gbq==0.15.0 --upgrade\n",
    "# ! pip3 install $USER google-cloud-secret-manager --upgrade\n",
    "# ! pip3 install $USER google-cloud-pubsub==2.13.4 --upgrade\n",
    "\n",
    "# import os\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     # Automatically restart kernel after installs\n",
    "#     import IPython\n",
    "\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)\n",
    "\n",
    "# # Check versions\n",
    "# ! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "# ! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load your pipeline components and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Related Python packages\n",
    "from icecream import ic\n",
    "from kfp.v2 import dsl\n",
    "\n",
    "# Components\n",
    "from ml_components.dataimport import (\n",
    "    get_rundates,\n",
    "    get_period,\n",
    "    get_import_query,\n",
    "    bq_query_no_return,\n",
    "    bq_query_to_dataframe,\n",
    "    get_previous_dataset_from_model_league,\n",
    "    \n",
    ")\n",
    "from ml_components.datacheck import (\n",
    "    printing,\n",
    "    grand_drift_check,\n",
    "    collect_check_result,\n",
    ")\n",
    "from ml_components.datapreproc import (\n",
    "    data_preprocess,\n",
    "    data_split,\n",
    ")\n",
    "from ml_components.modelling import (\n",
    "    get_champion_model,\n",
    "    candidate_generation_train,\n",
    "    candidate_generation_prediction,\n",
    "    ranking,\n",
    ")\n",
    "from ml_components.outbound import (\n",
    "    generate_bq_table_from_gsc,\n",
    "    update_model_league,\n",
    ")\n",
    "from ml_components.alert import (\n",
    "    push_slack_notification,\n",
    ")\n",
    "from ml_components.pipelinehelper import (\n",
    "    func_op,\n",
    "    save_pipeline,\n",
    "    run_pipeline,\n",
    ")\n",
    "\n",
    "# Configs\n",
    "from config import (\n",
    "    SERVICE_ACCOUNT,\n",
    "    PROJECT_ID,\n",
    "    REGION,\n",
    "    RUNNER,\n",
    "    PIPELINE_NAME,\n",
    "    BUCKET_NAME,\n",
    "    PIPELINE_ROOT,\n",
    "    USE_VAIEXP,\n",
    "    PARAMETER_VALUES,\n",
    "    parameter_checks\n",
    ")\n",
    "parameter_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your pipeline\n",
    "```\n",
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def your_pipeline(\n",
    "    parameter_value: data_type,\n",
    "):\n",
    "    ops_1 = func_op(\n",
    "        func=function,\n",
    "        _component_human_name='your ops 1 label',\n",
    "        base_image='python:3.7', #optional\n",
    "        function_arg=parameter_value,\n",
    "    )\n",
    "    \n",
    "    ops_2 = func_op(\n",
    "        func=function,\n",
    "        _component_human_name='your ops 2 label',\n",
    "        packages_to_install=[\n",
    "            'pandas==1.3.3'\n",
    "        ],\n",
    "        function_arg=ops_1.outputs['output'],\n",
    "    )\n",
    "```\n",
    "\n",
    "#### Choose your desired GCP pre-built image for your node(s)\n",
    "- https://cloud.google.com/deep-learning-containers/docs/choosing-container\n",
    "- `! gcloud container images list --repository=\"gcr.io/deeplearning-platform-release\"`\n",
    "- You can also just choose 'python:3.x' if want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def grand_pipeline(\n",
    "    bucket_name: str,\n",
    "    project_id: str,\n",
    "    project_name: str,\n",
    "    region: str,\n",
    "    job_id: str,\n",
    "    run_date: str,\n",
    "    seed: int,\n",
    "    embedding_dimension: int,\n",
    "    train_size: float,\n",
    "    catalog_col: str,\n",
    "    user_id_col: str,\n",
    "    product_score: str,\n",
    "    drop_columns: str,\n",
    "    numerical_columns: str,\n",
    "    feature_importance_dict_str: str,\n",
    "    numerical_drift_partition_threshold: float,\n",
    "    numerical_importance_partition_threshold: float,\n",
    "    categorical_drift_partition_threshold: float,\n",
    "    categorical_importance_partition_threshold: float,\n",
    "    category_threshold: int,\n",
    "    delta: int,\n",
    "    cg_model_params: str,\n",
    "    r_model_params: str,\n",
    "    is_endpoint: bool,\n",
    "    mlops_topic: str,\n",
    "    runner: str,\n",
    "    commit_short_sha: str,\n",
    "):\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "    ################################################### Ops Declaration ####################################################\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "\n",
    "    \n",
    "    ###########################################################\n",
    "    ######################### Rundate #########################\n",
    "    ###########################################################\n",
    "\n",
    "    rundates_op = func_op(\n",
    "        func=get_rundates,\n",
    "        _component_human_name='get_rundates',\n",
    "        base_image='python:3.7',\n",
    "        packages_to_install=['pandas==1.3.3'],\n",
    "        run_date=run_date,\n",
    "    )\n",
    "\n",
    "    usage_run_date = rundates_op.outputs['usage_run_date']\n",
    "    src_run_dt = rundates_op.outputs['src_run_dt']\n",
    "\n",
    "    ###########################################################\n",
    "    ####################### Import Data #######################\n",
    "    ###########################################################\n",
    "\n",
    "    period_op = func_op(\n",
    "        func=get_period,\n",
    "        _component_human_name='get_period',\n",
    "        base_image='python:3.7',\n",
    "        packages_to_install=['pandas==1.3.3'],\n",
    "        run_date=usage_run_date,\n",
    "    )\n",
    "\n",
    "    # Get queries\n",
    "    import_query_op = func_op(\n",
    "        func=get_import_query,\n",
    "        _component_human_name='get_import_query',\n",
    "        base_image='python:3.7',\n",
    "        start_date=period_op.outputs['train_start_date'],\n",
    "        end_date=period_op.outputs['train_end_date'],\n",
    "    )\n",
    "\n",
    "    # Get datasets using queries\n",
    "    user_dataset_op = func_op(\n",
    "        func=bq_query_to_dataframe,\n",
    "        _component_human_name='get_user_dataset',\n",
    "        base_image='python:3.7',\n",
    "        project_id=project_id,\n",
    "        cpu_limit='2',\n",
    "        memory_limit='32G',\n",
    "        query=import_query_op.outputs['user_query'],\n",
    "    )\n",
    "\n",
    "    history_dataset_op = func_op(\n",
    "        func=bq_query_to_dataframe,\n",
    "        _component_human_name='get_history_dataset',\n",
    "        base_image='python:3.7',\n",
    "        project_id=project_id,\n",
    "        cpu_limit='2',\n",
    "        memory_limit='32G',\n",
    "        query=import_query_op.outputs['history_query'],\n",
    "    )\n",
    "    catalog_dataset_op = func_op(\n",
    "        func=bq_query_to_dataframe,\n",
    "        _component_human_name='get_catalog_dataset',\n",
    "        base_image='python:3.7',\n",
    "        project_id=project_id,\n",
    "        cpu_limit='2',\n",
    "        memory_limit='32G',\n",
    "        query=import_query_op.outputs['catalog_query'],\n",
    "    )\n",
    "\n",
    "    # Get previous datasets using model league\n",
    "    prev_user_dataset_op = func_op(\n",
    "        func=get_previous_dataset_from_model_league,\n",
    "        _component_human_name='get_prev_user_dataset',\n",
    "        base_image='python:3.7',\n",
    "        packages_to_install=['pandas-gbq==0.15.0'],\n",
    "        cpu_limit='2',\n",
    "        memory_limit='32G',\n",
    "        project_id=project_id,\n",
    "        project_name=project_name,\n",
    "        dataset_type='user',\n",
    "        runner=runner,\n",
    "        location=region,\n",
    "    )\n",
    "    prev_catalog_dataset_op = func_op(\n",
    "        func=get_previous_dataset_from_model_league,\n",
    "        _component_human_name='get_prev_catalog_dataset',\n",
    "        base_image='python:3.7',\n",
    "        packages_to_install=['pandas-gbq==0.15.0'],\n",
    "        cpu_limit='2',\n",
    "        memory_limit='32G',\n",
    "        project_id=project_id,\n",
    "        project_name=project_name,\n",
    "        dataset_type='catalog',\n",
    "        runner=runner,\n",
    "        location=region,\n",
    "    )\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    #################### Data Preprocessing ###################\n",
    "    ###########################################################\n",
    "\n",
    "    history_data_preprocess_op = func_op(\n",
    "        func=data_preprocess,\n",
    "        _component_human_name='history_data_preprocess',\n",
    "        base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "        cpu_limit='4',\n",
    "        memory_limit='32G',\n",
    "        drop_columns=drop_columns,\n",
    "        numerical_columns=numerical_columns,\n",
    "        dataset=history_dataset_op.outputs['dataset'],\n",
    "    )\n",
    "\n",
    "    data_split_op = func_op(\n",
    "        func=data_split,\n",
    "        _component_human_name='data_split',\n",
    "        base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "        packages_to_install=[\n",
    "            'imbalanced-learn==0.9.0',\n",
    "        ],\n",
    "        cpu_limit='4',\n",
    "        memory_limit='32G',\n",
    "        seed=seed,\n",
    "        train_size=train_size,\n",
    "        processed_dataset=history_data_preprocess_op.outputs['processed_dataset'],\n",
    "    )\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    #################### Data Drift Check #####################\n",
    "    ###########################################################\n",
    "\n",
    "    user_drift_check_op = func_op(\n",
    "        func=grand_drift_check,\n",
    "        _component_human_name='user_drift_check',\n",
    "        base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "        cpu_limit='4',\n",
    "        memory_limit='32G',\n",
    "        project_name=project_name,\n",
    "        dataset_p=user_dataset_op.outputs['dataset'],\n",
    "        dataset_q=prev_user_dataset_op.outputs['previous_dataset'],\n",
    "        feature_importance_dict_str=feature_importance_dict_str,\n",
    "        numerical_drift_partition_threshold=numerical_drift_partition_threshold,\n",
    "        numerical_importance_partition_threshold=numerical_importance_partition_threshold,\n",
    "        categorical_drift_partition_threshold=categorical_drift_partition_threshold,\n",
    "        categorical_importance_partition_threshold=categorical_importance_partition_threshold,\n",
    "        category_threshold=category_threshold,\n",
    "        delta=delta,\n",
    "        mode='user',\n",
    "    )\n",
    "    catalog_drift_check_op = func_op(\n",
    "        func=grand_drift_check,\n",
    "        _component_human_name='catalog_drift_check',\n",
    "        base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "        cpu_limit='4',\n",
    "        memory_limit='32G',\n",
    "        project_name=project_name,\n",
    "        dataset_p=catalog_dataset_op.outputs['dataset'],\n",
    "        dataset_q=prev_catalog_dataset_op.outputs['previous_dataset'],\n",
    "        feature_importance_dict_str=feature_importance_dict_str,\n",
    "        numerical_drift_partition_threshold=numerical_drift_partition_threshold,\n",
    "        numerical_importance_partition_threshold=numerical_importance_partition_threshold,\n",
    "        categorical_drift_partition_threshold=categorical_drift_partition_threshold,\n",
    "        categorical_importance_partition_threshold=categorical_importance_partition_threshold,\n",
    "        category_threshold=category_threshold,\n",
    "        delta=delta,\n",
    "        mode='catalog',\n",
    "    )\n",
    "    collect_check_result_op = func_op(\n",
    "        func=collect_check_result,\n",
    "        _component_human_name='collect_check_result',\n",
    "        base_image='python:3.7',\n",
    "        cpu_limit='2',\n",
    "        memory_limit='16G',\n",
    "        user_drift_check=user_drift_check_op.outputs['drift_status'],\n",
    "        catalog_drift_check=catalog_drift_check_op.outputs['drift_status'],\n",
    "    )\n",
    "\n",
    "    ###########################################################\n",
    "    ######################## Modelling ########################\n",
    "    ###########################################################\n",
    "\n",
    "    with dsl.Condition(\n",
    "        collect_check_result_op.outputs['retrain_model'] == 'true',\n",
    "        name=\"modelling\"\n",
    "    ):\n",
    "    #     data_check_result_fail_op = func_op(\n",
    "    #         func=push_slack_notification,\n",
    "    #         _component_human_name='data_check_result_fail',\n",
    "    #         base_image='python:3.7',\n",
    "    #         packages_to_install=['google-cloud-secret-manager'],\n",
    "    #         job_id=job_id,\n",
    "    #         src_run_dt=src_run_dt,\n",
    "    #         text=data_check_result_op.outputs['alert_msg'],\n",
    "    #         channel='#your-project-name-internal',\n",
    "    #         webhook_config_str=failure_webhook_config,\n",
    "    #         runner=runner,\n",
    "    #     )\n",
    "\n",
    "        user_dataset_to_bq_op = func_op(\n",
    "            func=generate_bq_table_from_gsc,\n",
    "            _component_human_name='user_dataset_to_bq',\n",
    "            base_image='python:3.7',\n",
    "            cpu_limit='2',\n",
    "            memory_limit='8G',\n",
    "            retry=3,\n",
    "            project_id=project_id,\n",
    "            project_name=project_name,\n",
    "            dataset_id='MLOPS_TRAIN_DATASET',\n",
    "            runner=runner,\n",
    "            table_name='user',\n",
    "            src_run_dt=src_run_dt,\n",
    "            dataset_format='PARQUET',\n",
    "            location=region,\n",
    "            dataset_to_save=user_dataset_op.outputs['dataset'],\n",
    "        )\n",
    "        catalog_dataset_to_bq_op = func_op(\n",
    "            func=generate_bq_table_from_gsc,\n",
    "            _component_human_name='catalog_dataset_to_bq',\n",
    "            base_image='python:3.7',\n",
    "            cpu_limit='2',\n",
    "            memory_limit='8G',\n",
    "            retry=3,\n",
    "            project_id=project_id,\n",
    "            project_name=project_name,\n",
    "            dataset_id='MLOPS_TRAIN_DATASET',\n",
    "            runner=runner,\n",
    "            table_name='catalog',\n",
    "            src_run_dt=src_run_dt,\n",
    "            dataset_format='PARQUET',\n",
    "            location=region,\n",
    "            dataset_to_save=catalog_dataset_op.outputs['dataset'],\n",
    "        )\n",
    "        train_dataset_to_bq_op = func_op(\n",
    "            func=generate_bq_table_from_gsc,\n",
    "            _component_human_name='train_dataset_to_bq',\n",
    "            base_image='python:3.7',\n",
    "            cpu_limit='2',\n",
    "            memory_limit='8G',\n",
    "            retry=3,\n",
    "            project_id=project_id,\n",
    "            project_name=project_name,\n",
    "            dataset_id='MLOPS_TRAIN_DATASET',\n",
    "            runner=runner,\n",
    "            table_name='train',\n",
    "            src_run_dt=src_run_dt,\n",
    "            dataset_format='PARQUET',\n",
    "            location=region,\n",
    "            dataset_to_save=data_split_op.outputs['train_dataset'],\n",
    "        )\n",
    "\n",
    "        model_champion_op = func_op(\n",
    "            func=get_champion_model,\n",
    "            _component_human_name='get_champion_model',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            packages_to_install=[\n",
    "                'pandas-gbq==0.15.0',\n",
    "            ],\n",
    "            project_id=project_id,\n",
    "            project_name=project_name,\n",
    "            runner=runner,\n",
    "            location=region,\n",
    "        )\n",
    "\n",
    "        candidate_generation_train_op = func_op(\n",
    "            func=candidate_generation_train,\n",
    "            _component_human_name='candidate_generation_train',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            packages_to_install=[\n",
    "                'tensorflow==2.9.1',\n",
    "                'tensorflow-recommenders==0.6.0',\n",
    "                'scann==1.2.7',\n",
    "            ],\n",
    "            cpu_limit='4',\n",
    "            memory_limit='32G',\n",
    "            is_cold_start=model_champion_op.outputs['is_cold_start'],\n",
    "            bucket_name=bucket_name,\n",
    "            model_params=cg_model_params,\n",
    "            user_id_col=user_id_col,\n",
    "            catalog_col=catalog_col,\n",
    "            embedding_dimension=embedding_dimension,\n",
    "            champion_embedding_dimension=model_champion_op.outputs['embedding_dimension'],\n",
    "            cg_model_weight_path=model_champion_op.outputs['cg_model_weight_path'],\n",
    "            catalog_dataset=catalog_dataset_op.outputs['dataset'],\n",
    "            train_dataset=data_split_op.outputs['train_dataset'],\n",
    "            test_dataset=data_split_op.outputs['test_dataset'],\n",
    "            champion_unique_catalog_ids_dataset=model_champion_op.outputs['champion_unique_catalog_ids_dataset'],\n",
    "            champion_unique_user_ids_dataset=model_champion_op.outputs['champion_unique_user_ids_dataset'],\n",
    "\n",
    "        )\n",
    "\n",
    "        candidate_generation_prediction_op = func_op(\n",
    "            func=candidate_generation_prediction,\n",
    "            _component_human_name='candidate_generation_prediction',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            packages_to_install=[\n",
    "                'tensorflow==2.9.1',\n",
    "                'tensorflow-recommenders==0.6.0',\n",
    "                'scann==1.2.7',\n",
    "                'joblib==1.1.0',\n",
    "                'tqdm==4.63.0',\n",
    "            ],\n",
    "            cpu_limit='4',\n",
    "            memory_limit='32G',\n",
    "            user_id_col=user_id_col,\n",
    "            catalog_col=catalog_col,\n",
    "            model_params=cg_model_params,\n",
    "            user_dataset=user_dataset_op.outputs['dataset'],\n",
    "            cg_index_object=candidate_generation_train_op.outputs['cg_index_object'],\n",
    "        )\n",
    "\n",
    "        ranking_op = func_op(\n",
    "            func=ranking,\n",
    "            _component_human_name='ranking',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            packages_to_install=[\n",
    "                'tensorflow==2.9.1',\n",
    "                'tensorflow-recommenders==0.6.0',\n",
    "                'joblib==1.1.0',\n",
    "                'tqdm==4.63.0',\n",
    "            ],\n",
    "            cpu_limit='4',\n",
    "            memory_limit='32G',\n",
    "            is_endpoint=is_endpoint,\n",
    "            best_model=candidate_generation_train_op.outputs['best_model'],\n",
    "            model_params=r_model_params,\n",
    "            user_id_col=user_id_col,\n",
    "            catalog_col=catalog_col,\n",
    "            product_score=product_score,\n",
    "            chosen_embedding_dimension=candidate_generation_train_op.outputs['chosen_embedding_dimension'],\n",
    "            chosen_unique_catalog_ids_dataset=candidate_generation_train_op.outputs['chosen_unique_catalog_ids_dataset'],\n",
    "            chosen_unique_user_ids_dataset=candidate_generation_train_op.outputs['chosen_unique_user_ids_dataset'],\n",
    "            train_dataset=data_split_op.outputs['train_dataset'],\n",
    "            test_dataset=data_split_op.outputs['test_dataset'],\n",
    "            cg_results_dataset=candidate_generation_prediction_op.outputs['cg_results_dataset'],\n",
    "        )\n",
    "\n",
    "\n",
    "        ###########################################################\n",
    "        ######################## Outbound #########################\n",
    "        ###########################################################\n",
    "\n",
    "        update_model_league_op = func_op(\n",
    "            func=update_model_league,\n",
    "            _component_human_name='update_model_league',\n",
    "            base_image='gcr.io/deeplearning-platform-release/sklearn-cpu',\n",
    "            packages_to_install=[\n",
    "                'pandas-gbq==0.15.0',\n",
    "                'google-cloud-pubsub==2.13.0',\n",
    "            ],\n",
    "            cpu_limit='4',\n",
    "            memory_limit='16G',\n",
    "            project_id=project_id,\n",
    "            project_name=project_name,\n",
    "            location=region,\n",
    "            job_id=job_id,\n",
    "            src_run_dt=src_run_dt,\n",
    "            user_bq_path=user_dataset_to_bq_op.outputs['bq_path'],\n",
    "            catalog_bq_path=catalog_dataset_to_bq_op.outputs['bq_path'],\n",
    "            train_bq_path=train_dataset_to_bq_op.outputs['bq_path'],\n",
    "            chosen_embedding_dimension=candidate_generation_train_op.outputs['chosen_embedding_dimension'],\n",
    "            chosen_model=candidate_generation_train_op.outputs['best_model'],\n",
    "            runner=runner,\n",
    "            commit_short_sha=commit_short_sha,\n",
    "            is_cold_start=model_champion_op.outputs['is_cold_start'],\n",
    "            is_endpoint=is_endpoint,\n",
    "            mlops_topic=mlops_topic,\n",
    "            chosen_unique_catalog_ids_dataset=candidate_generation_train_op.outputs['chosen_unique_catalog_ids_dataset'],\n",
    "            chosen_unique_user_ids_dataset=candidate_generation_train_op.outputs['chosen_unique_user_ids_dataset'],\n",
    "            cg_model_object=candidate_generation_train_op.outputs['cg_model_object'],\n",
    "            cg_index_object=candidate_generation_train_op.outputs['cg_index_object'],\n",
    "            r_model_object=ranking_op.outputs['r_model_object'],\n",
    "        )\n",
    "\n",
    "\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "    ##################################################### Ops Caching ######################################################\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "\n",
    "    nodes_no_cache = [\n",
    "        rundates_op,\n",
    "\n",
    "#         period_op,\n",
    "#         import_query_op,\n",
    "\n",
    "#         user_dataset_op,\n",
    "#         history_dataset_op,\n",
    "#         catalog_dataset_op,\n",
    "\n",
    "#         history_data_preprocess_op,\n",
    "#         data_split_op,\n",
    "\n",
    "        prev_user_dataset_op,\n",
    "        prev_catalog_dataset_op,\n",
    "\n",
    "#         user_drift_check_op,\n",
    "#         catalog_drift_check_op,\n",
    "#         collect_check_result_op\n",
    "\n",
    "#         user_dataset_to_bq_op,\n",
    "#         catalog_dataset_to_bq_op,\n",
    "#         train_dataset_to_bq_op,\n",
    "\n",
    "        model_champion_op,\n",
    "\n",
    "#         candidate_generation_train_op,\n",
    "#         candidate_generation_prediction_op,\n",
    "#         ranking_op,\n",
    "\n",
    "        update_model_league_op,\n",
    "    ]\n",
    "    \n",
    "    if nodes_no_cache:\n",
    "        for node in nodes_no_cache:\n",
    "            node.set_caching_options(enable_caching=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1295: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_spec/usecase-recsys-dev-2022-08-11-t-11-04-29.json uploaded to GCS.\n",
      "/tmp/usecase-recsys-dev-2022-08-11-t-11-04-29.json removed in local.\n"
     ]
    }
   ],
   "source": [
    "TEMPLATE_PATH, JOB_ID, DISPLAY_NAME = save_pipeline(\n",
    "    pipeline=grand_pipeline,\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    bucket_name=BUCKET_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your pipeline locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/751015570376/locations/us-central1/pipelineJobs/usecase-recsys-dev-2022-08-11-t-11-04-29\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/751015570376/locations/us-central1/pipelineJobs/usecase-recsys-dev-2022-08-11-t-11-04-29')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/usecase-recsys-dev-2022-08-11-t-11-04-29?project=751015570376\n",
      "Associating projects/751015570376/locations/us-central1/pipelineJobs/usecase-recsys-dev-2022-08-11-t-11-04-29 to Experiment: usecase-recsys-dev-experiment\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(\n",
    "    project_id=PROJECT_ID,\n",
    "    staging_bucket=BUCKET_NAME,\n",
    "    location=REGION,\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=TEMPLATE_PATH,\n",
    "    job_id=JOB_ID,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    parameter_values=PARAMETER_VALUES,\n",
    "    # enable_caching=False,\n",
    "    use_vaiexp=USE_VAIEXP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lightweight_functions_component_io_kfp.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
